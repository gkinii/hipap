import torch
import torch.nn as nn
import torch.nn.functional as F


class MultimodalityInduction(nn.Module):
    """Enables the model to forward and predict multi-mode predictions.
    
    1) Features are broadcasted to number of modes and summed with learned mode
       tensors.
    2) Mixture Weights are generated by cross-attention over all dimensions
       between learned mode tensors and hidden tensors.
    """
    
    def __init__(
        self,
        params,
        hidden_size=256,
    ):
        super().__init__()
        self.num_modes = params.num_modes
        
        if hidden_size % params.num_heads != 0:
            raise ValueError(
                f'hidden_size ({hidden_size}) must be an integer '
                f'times bigger than num_heads ({params.num_heads}).'
            )
        
        # Multi-mode attention layer (attention over modes dimension)
        self.mm_attn_layer = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=params.num_heads,
            dropout=params.drop_prob,
            batch_first=True
        )
        self.mm_attn_ln = nn.LayerNorm(hidden_size)
        
        # Multi-mode feed-forward layers
        self.mm_ff_layer1 = nn.Linear(hidden_size, params.ff_dim)
        self.mm_ff_layer2 = nn.Linear(params.ff_dim, hidden_size)
        self.mm_ff_ln = nn.LayerNorm(hidden_size)
        
        # Mixture weights attention layer (cross-attention)
        self.mw_attn_layer = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=params.num_heads,
            dropout=0.0,
            batch_first=True
        )
        self.mw_attn_ln = nn.LayerNorm(hidden_size)
        
        # Mixture weights feed-forward layers
        self.mw_ff_layer1 = nn.Linear(hidden_size, params.ff_dim)
        self.mw_ff_layer2 = nn.Linear(params.ff_dim, 1)  # Single logit per mode
        self.mw_ff_ln = nn.LayerNorm(hidden_size)
        
        # Dropout layers
        self.attn_dropout = nn.Dropout(params.drop_prob)
        self.ff_dropout = nn.Dropout(params.drop_prob)
        
        # Learned mode embeddings: [1, 1, 1, num_modes, hidden_size]
        self.learned_add_mm = nn.Parameter(
            torch.empty(1, 1, 1, self.num_modes, hidden_size).uniform_(-1.0, 1.0)
        )
    
    def forward(self, input_batch):
        input_batch = {k: v for k, v in input_batch.items()}
        
        # [b, a, t, h] -> [b, a, t, 1, h]
        hidden_vecs = input_batch['hidden_vecs'].unsqueeze(-2)
        
        # Multi Modes: broadcast and add learned mode embeddings
        # hidden_vecs: [b, a, t, 1, h]
        # learned_add_mm: [1, 1, 1, n, h]
        # mm_add: [b, a, t, n, h]
        mm_add = self.mm_attn_ln(self.learned_add_mm + hidden_vecs)
        
        # For attention over modes dimension, we need to reshape
        # We want attention across the modes (dimension 3)
        b, a, t, n, h = mm_add.shape

        mm_add_reshaped = mm_add.reshape(b * a * t, n, h)
        
        # Self-attention over modes
        attn_out_mm, _ = self.mm_attn_layer(
            mm_add_reshaped,
            mm_add_reshaped,
            mm_add_reshaped
        )
        
        # Reshape back
        attn_out_mm = attn_out_mm.reshape(b, a, t, n, h)
        out = attn_out_mm + hidden_vecs
        # Feed-forward layers for multi-mode
        
        out = F.relu(self.mm_ff_layer1(out))
        out = self.mm_ff_layer2(out)
        out = self.ff_dropout(out)

        out = self.mm_ff_ln(out + hidden_vecs)
        
        # Update hidden_vecs with modes dimension
        input_batch['hidden_vecs'] = out
        
        # Mixture Weights generation using cross-attention
        # Query: learned mode embeddings [1, 1, 1, n, h]
        # Key/Value: mm_add [b, a, t, n, h]
        
        # Tile learned embeddings to match batch size

        learned_mm_tiled = self.learned_add_mm.expand(b, -1, -1, -1, -1)

        # For cross-attention, we need to reshape appropriately
        # We want to attend from each mode embedding to all spatial-temporal-mode positions
        
        # Reshape for cross-attention
        # Query: [b, n, h] (one query per mode)
        # Key/Value: [b, a*t*n, h] (all positions and modes)
        query = learned_mm_tiled.reshape(b, n, h)

        key_value = mm_add.reshape(b, a * t * n, h)
        
        # Cross-attention
        attn_out_mw, _ = self.mw_attn_layer(
            query,
            key_value,
            key_value
        )
        
        # attn_out_mw: [b, n, h]
        attn_out_mw = self.attn_dropout(attn_out_mw)
        
        # Reshape to match expected shape [b, 1, 1, n, h]
        attn_out_mw = attn_out_mw.reshape(b, 1, 1, n, h)
        attn_out_mw = self.mw_attn_ln(attn_out_mw)
        
        # Feed-forward layers for mixture weights
        
        out_mw = F.relu(self.mw_ff_layer1(attn_out_mw))
        out_mw = self.mw_ff_layer2(out_mw)  # [*, 1]
        
        out_mw = self.ff_dropout(out_mw).squeeze(-1)
        
        return out, out_mw